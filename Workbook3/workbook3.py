"""
Note: this was origianlly an ipynb but the file size was too large
to upload to GitHub so that's why I'm saving it as a .py file
"""

# -*- coding: utf-8 -*-
"""workbook3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IM76SsBgsTPmbM1tJ0DSfbanRpJOl5mT

# Install and load required libraries
"""

#!pip install miditok
#!pip install symusic
#!pip install glob
#!pip install torch

import random
from numpy.random import choice
from miditok import REMI, TokenizerConfig
import glob
import torch
from torch.utils.data import Dataset, DataLoader
from symusic import Score
import numpy as np
from typing import List
from collections import defaultdict

"""# Markov Chains for Next Chord Prediction
(Ignore sections, bar lines, etc.)
"""

a = open("data/chords.json")
dataset = []
for l in a.readlines():
    d = eval(l)
    dataset.append(d)

len(dataset)

dataset[0]

flatDataset = []
for d in dataset:
    flat = []
    for part in d['chords']:
        for bar in part:
            flat += bar
    flatDataset.append(flat)

flatDataset[0]

unigrams = defaultdict(int)
bigrams = defaultdict(int)

for d in flatDataset:
    for chord in d:
        unigrams[chord] += 1
    for (chord1,chord2) in zip(d[:-1],d[1:]):
        bigrams[(chord1,chord2)] += 1

unigramCounts = [(unigrams[k],k) for k in unigrams]
bigramCounts = [(bigrams[k],k) for k in bigrams]

unigramCounts.sort()
bigramCounts.sort()

unigramCounts[-10:]

bigramCounts[-10:]

dictionary = set(flatDataset[3])

"""## Compute transition probabilities"""

transitions = defaultdict(list)
transitionProbabilities = defaultdict(list)

for b1,b2 in bigrams:
    if b1 in dictionary and b2 in dictionary:
        transitions[b1].append(b2)
        transitionProbabilities[b1].append(bigrams[(b1,b2)])

transitions

transitionProbabilities

def sample(length):
    seq = [random.choice(list(transitionProbabilities.keys()))]
    while len(seq) < length:
        probs = np.array(transitionProbabilities[seq[-1]])
        if not np.isclose(probs.sum(), 1.0):
            probs = probs / probs.sum()
        nextchord = choice(transitions[seq[-1]], 1, p=probs)
        seq.append(nextchord.item())
    return seq

sample(10)

"""# Markov Chain for MIDI generation
## Get the list of files for training and test sets
"""

train_files = glob.glob("./data/train/*.mid")
test_files = glob.glob("./data/test/*.mid")

"""## Train your MIDI tokenizer"""

config = TokenizerConfig(num_velocities=1, use_chords=False, use_programs=True)
tokenizer = REMI(config)
tokenizer.train(vocab_size=1000, files_paths=train_files)
tokenizer.save("tokenizer.json")

"""## Construct a PyTorch Dataset"""

class MIDIDataset(Dataset):
    def __init__(self, file_paths: List[str], tokenizer):
        self.tokenizer = tokenizer
        self.file_paths = file_paths
    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        midi = Score(self.file_paths[idx])
        tokens = self.tokenizer(midi)
        return np.array(tokens)

"""## Define PyTorch datasets and dataloaders"""

train_dataset = MIDIDataset(train_files, tokenizer)
test_dataset = MIDIDataset(test_files, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

"""## Define a Second Order Markov Chain model"""

class SecondOrderMarkovChain:
    def __init__(self):
        self.transitions = defaultdict(lambda: defaultdict(int))
        self.probabilities = defaultdict(lambda: defaultdict(float))

    def train(self, train_loader):
        for sequence in train_loader:
            sequence = sequence[0].numpy().astype(int)
            for i in range(len(sequence) - 2):
                state1, state2 = sequence[i], sequence[i + 1]
                next_state = sequence[i + 2]
                self.transitions[(state1, state2)][next_state] += 1

        for (state1, state2), next_states in self.transitions.items():
            total = sum(next_states.values())
            for next_state, count in next_states.items():
                self.probabilities[(state1, state2)][next_state] = count / total
        return self.probabilities

    def generate(self, test_sequence, num_predictions=1):
        test_sequence = test_sequence[0].numpy().astype(int)
        results = [test_sequence[0], test_sequence[1]]
        for i in range(100):
            if (results[-2], results[-1]) not in self.probabilities:
                break
            else:
                probs = self.probabilities[(results[-2], results[-1])]
                states = list(probs.keys())
                probabilities = list(probs.values())
                if not states:
                    break
                try:
                    predictions = np.random.choice(states, size=num_predictions, p=probabilities)
                except:
                    break
                results.append(predictions[0])
        return results

"""## Train your model and make inferences"""

model = SecondOrderMarkovChain()
model.train(train_loader)

predictions = []
for test_sequence in test_loader:
    predictions.append(model.generate(test_sequence))
for i, prediction in enumerate(predictions):
    output_score = tokenizer.decode(torch.Tensor(prediction))
    output_score.dump_midi(f"{i}.mid")

#!brew install wget
#!wget https://raw.githubusercontent.com/musescore/MuseScore/master/share/sound/FluidR3Mono_GM.sf3
#!pip install midi2audio
#!pip install IPython

from midi2audio import FluidSynth # Import library
from IPython.display import Audio, display
fs = FluidSynth("FluidR3Mono_GM.sf3") # Initialize FluidSynth
for i in range(len(predictions)):
    fs.midi_to_audio(f"{i}.mid", f"{i}.wav")
    display(Audio(f"{i}.wav"))
