{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b313108-d89f-4f1a-a2c8-ef0a32bb282c",
   "metadata": {},
   "source": [
    "Midi file player\n",
    "https://midiplayer.ehubsoft.net/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0bc8d0-897c-4181-a900-50a4286597d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecedfc3-d937-47cd-a163-920d50a1fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa | tail -n 1  # I don't want a super long output\n",
    "# !pip install miditoolkit | tail -n 1  # I don't want a super long output\n",
    "# !pip install xgboost | tail -n 1  # I don't want a super long output\n",
    "# !pip install lightgbm | tail -n 1  # I don't want a super long output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463821d2-7f73-4acf-ad87-ed90f202d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably more imports than are really necessary...\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import miditoolkit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import random\n",
    "\n",
    "# from mido import MidiFile\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from music21 import converter, chord, stream\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36636021-411e-474b-977d-07d1ecb1cffc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# How average precision score works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125336b7-cf33-4b1f-9174-8f048a05a951",
   "metadata": {},
   "source": [
    "Suppose prediction = [1 1 0 1], target = [1 0 0 0]  \n",
    "Precision at rank 1 = 1 / 1 = 1.0  \n",
    "Precision at rank 2 = 1 /2 = 0.5  \n",
    "Precision at rank 3 = 2 / 3 = 0.67  \n",
    "Precision at rank 4 = 2 / 4 = 0.5  \n",
    "So the Average Precision (AP) of my prediction and target is (1.0 + 0.5 + 0.67 + 0.5) / 4\n",
    "\n",
    "For 2D prediction and target arrays like in accuracy3, the average precision for the entire set is\n",
    "the average of the average precision values of each instance (or pair of prediction and ground truth).\n",
    "\n",
    "Suppose  \n",
    "predictions = [  \n",
    "    [1, 1],  # Instance 1 predictions  \n",
    "    [0, 1]   # Instance 2 predictions  \n",
    "]  \n",
    "groundtruth = [  \n",
    "    [1, 0],  # Instance 1 ground truth  \n",
    "    [1, 1]   # Instance 2 ground truth  \n",
    "]   \n",
    "AP for instance 1 = (1.0 + 0.5) / 2 = 0.75  \n",
    "AP for instance 2 = (0.0 + 0.5) / 2 = 0.25  \n",
    "So the mean Average Precision (mAP) is (0.75 + 0.25) / 2 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af0834-5106-4a82-bb1e-066bda70bb58",
   "metadata": {},
   "source": [
    "# Model: CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716945a4-ba18-4668-9aee-2744b5af1d3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tags and Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b372100f-02bd-4653-8c88-07cf00bf521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = ['rock', 'oldies', 'jazz', 'pop', 'dance',  'blues',  'punk', 'chill', 'electronic', 'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf31e71-a477-4088-8982-63168e59a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy3(groundtruth, predictions):\n",
    "    preds, targets = [], []\n",
    "    for k in groundtruth:\n",
    "        if not (k in predictions):\n",
    "            print(\"Missing \" + str(k) + \" from predictions\")\n",
    "            return 0\n",
    "        prediction = [1 if tag in predictions[k] else 0 for tag in TAGS]\n",
    "        target = [1 if tag in groundtruth[k] else 0 for tag in TAGS]\n",
    "        preds.append(prediction)\n",
    "        targets.append(target)\n",
    "\n",
    "    mAP = average_precision_score(targets, preds, average='macro')\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eec637-a77c-496b-965e-fffc24ad4e9a",
   "metadata": {},
   "source": [
    "### Getting Waveform Data for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0b7685d-9ee7-4a65-924e-5800d7416dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot3 = \"student_files/task3_audio_classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39960b0c-eb9c-498a-a4fd-ffd47845a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_waveform(path, pitch_shift_steps=0):\n",
    "    waveform, sr = librosa.load(dataroot3 + path, sr=SAMPLE_RATE)\n",
    "\n",
    "    # Apply pitch shift for data augmentation\n",
    "    if pitch_shift_steps != 0:\n",
    "        waveform = librosa.effects.pitch_shift(waveform, sr=sr, n_steps=pitch_shift_steps)\n",
    "\n",
    "    waveform = np.array([waveform])\n",
    "    if sr != SAMPLE_RATE:\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "        waveform = resample(waveform)\n",
    "    # Pad so that everything is the right length\n",
    "    target_len = SAMPLE_RATE * AUDIO_DURATION\n",
    "    # waveform = torch.from_numpy(waveform)  # Adding this because I was getting TypeError: pad(): argument 'input' (position 1) must be Tensor, not numpy.ndarray\n",
    "    if waveform.shape[1] < target_len:\n",
    "        print(\"padding\")\n",
    "        pad_len = target_len - waveform.shape[1]\n",
    "        waveform = F.pad(waveform, (0, pad_len))\n",
    "    else:\n",
    "        waveform = waveform[:, :target_len]\n",
    "    waveform = torch.FloatTensor(waveform)\n",
    "    # print(waveform.shape[1])\n",
    "    if waveform.shape[1] != target_len:  # 160000\n",
    "        print(\"error\")\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556e7ad-0f00-4ae8-b27a-a029d393f040",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dad36001-19d9-4c8d-8569-a3a09c6a8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants (you can change any of these if useful)\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 64\n",
    "N_CLASSES = 10\n",
    "AUDIO_DURATION = 10 # seconds\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ce9028-71dc-4683-a674-2a7d31b918e7",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c76f591e-fa2a-4815-b51c-db7f4166ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, meta):\n",
    "        self.meta = meta  # e.g. 'train/0.wav': ['rock']\n",
    "        ks = list(meta.keys())\n",
    "        self.idToPath = dict(zip(range(len(ks)), ks))  # e.g. 0: 'train/0.wav'\n",
    "        self.pathToFeat = {}\n",
    "\n",
    "        self.mel = MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=N_MELS)\n",
    "        self.db = AmplitudeToDB()\n",
    "\n",
    "        for path in ks:\n",
    "            waveforms = extract_waveform(path)\n",
    "            mel_spec = self.db(self.mel(waveform)).squeeze(0)\n",
    "            self.pathToFeat[path] = mel_spec  # e.g. 'train/0.wav': <Tensor shape=[N_MELS=64, T0]>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.idToPath[idx]\n",
    "        tags = self.meta[path]\n",
    "        bin_label = torch.tensor([1 if tag in tags else 0 for tag in TAGS], dtype=torch.float32)\n",
    "    \n",
    "        if self.preload:\n",
    "            mel_spec = self.pathToFeat[path]\n",
    "            # No pitch shift for preloaded\n",
    "            return mel_spec.unsqueeze(0), bin_label, path\n",
    "        else:\n",
    "            # Original waveform\n",
    "            waveform_orig = extract_waveform(path)\n",
    "            mel_spec_orig = self.db(self.mel(waveform_orig)).squeeze(0)\n",
    "    \n",
    "            # Pitch shifted version (within an octave up or down)\n",
    "            pitch_shift_steps = random.choice([i for i in range(-11, 12) if i != 0])  ## exclude 0 (no pitch change) \n",
    "            waveform_aug = extract_waveform(path, pitch_shift_steps=pitch_shift_steps)\n",
    "            mel_spec_aug = self.db(self.mel(waveform_aug)).squeeze(0)\n",
    "    \n",
    "            # Return both (could also randomize whether to return original or augmented only)\n",
    "            return (mel_spec_orig.unsqueeze(0), mel_spec_aug.unsqueeze(0)), bin_label, path\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     # Faster version, preloads the features\n",
    "    #     path = self.idToPath[idx]\n",
    "    #     tags = self.meta[path]\n",
    "    #     bin_label = torch.tensor([1 if tag in tags else 0 for tag in TAGS], dtype=torch.float32)\n",
    "\n",
    "    #     if self.preload:\n",
    "    #         mel_spec = self.pathToFeat[path]\n",
    "    #     else:\n",
    "    #         waveform = extract_waveform(path)\n",
    "    #         mel_spec = self.db(self.mel(waveform)).squeeze(0)\n",
    "\n",
    "    #     return mel_spec.unsqueeze(0), bin_label, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "994b3d29-7ac8-40ca-9a23-82171e9e1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loaders():\n",
    "    def __init__(self, train_path, test_path, split_ratio=0.9, seed = 0):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        meta_train = eval(open(train_path, 'r').read())\n",
    "        l_test = eval(open(test_path, 'r').read())\n",
    "        meta_test = dict([(x,[]) for x in l_test]) # Need a dictionary for the above class\n",
    "\n",
    "        all_train = AudioDataset(meta_train)\n",
    "        test_set = AudioDataset(meta_test)\n",
    "\n",
    "        # Split all_train into train + valid\n",
    "        total_len = len(all_train)\n",
    "        train_len = int(total_len * split_ratio)\n",
    "        valid_len = total_len - train_len\n",
    "        train_set, valid_set = random_split(all_train, [train_len, valid_len])\n",
    "\n",
    "        self.loaderTrain = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        self.loaderValid = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        self.loaderTest = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04fd58cc-5053-4a94-b8be-dbd1ee880b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=N_CLASSES):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(32 * (N_MELS // 4) * (801 // 4), 256)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (B, 16, mel/2, time/2)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (B, 32, mel/4, time/4)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return torch.sigmoid(self.fc2(x))  # multilabel → sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b34f6d9-aeff-4a79-87b9-c80c02a2f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self, model, learning_rate, seed = 0):\n",
    "        # These two lines will (mostly) make things deterministic.\n",
    "        # You're welcome to modify them to try to get a better solution.\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # self.device = torch.device(\"cpu\") # Can change this if you have a GPU, but the autograder will use CPU\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device) #model.cuda() # Also uncomment these lines for GPU\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def evaluate(self, loader, threshold=0.5, outpath=None):\n",
    "        self.model.eval()\n",
    "        preds, targets, paths = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, ps in loader:\n",
    "                x = x.to(self.device) #x.cuda()\n",
    "                y = y.to(self.device) #y.cuda()\n",
    "                outputs = self.model(x)\n",
    "                preds.append(outputs.cpu())\n",
    "                targets.append(y.cpu())\n",
    "                paths += list(ps)\n",
    "\n",
    "        preds = torch.cat(preds)\n",
    "        targets = torch.cat(targets)\n",
    "        preds_bin = (preds > threshold).float()\n",
    "\n",
    "        predictions = {}\n",
    "        for i in range(preds_bin.shape[0]):\n",
    "            predictions[paths[i]] = [TAGS[j] for j in range(len(preds_bin[i])) if preds_bin[i][j]]\n",
    "\n",
    "        mAP = None\n",
    "        if outpath: # Save predictions\n",
    "            with open(outpath, \"w\") as z:\n",
    "                z.write(str(predictions) + '\\n')\n",
    "        else: # Only compute accuracy if we're *not* saving predictions, since we can't compute test accuracy\n",
    "            mAP = average_precision_score(targets, preds, average='macro')\n",
    "        return predictions, mAP\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for (x1, x2), y, path in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "                x1 = x1.to(self.device)\n",
    "                x2 = x2.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "            \n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "                # Forward both versions and average the results\n",
    "                outputs1 = self.model(x1)\n",
    "                outputs2 = self.model(x2)\n",
    "                outputs = (outputs1 + outputs2) / 2\n",
    "            \n",
    "                loss = self.criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            # for x, y, path in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            #     x = x.to(self.device) #x.cuda()\n",
    "            #     y = y.to(self.device) #y.cuda()\n",
    "            #     self.optimizer.zero_grad()\n",
    "            #     outputs = self.model(x)\n",
    "            #     loss = self.criterion(outputs, y)\n",
    "            #     loss.backward()\n",
    "            #     self.optimizer.step()\n",
    "            #     running_loss += loss.item()\n",
    "            val_predictions, mAP = self.evaluate(val_loader)\n",
    "            print(f\"[Epoch {epoch+1}] Loss: {running_loss/len(train_loader):.4f} | Val mAP: {mAP:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fadbc760-8661-459d-897c-68a709bfc518",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = Loaders(dataroot3 + \"/train.json\", dataroot3 + \"/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "647a76f9-ced5-4cbb-8c3b-08a3d258d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56a9d078-979d-46ba-93eb-af568ca6e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/113 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(model, \u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloaderTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloaderValid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[61], line 46\u001b[0m, in \u001b[0;36mPipeline.train\u001b[0;34m(self, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     45\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x1, x2), y, path \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     47\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x1\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     48\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m x2\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(model, 1e-4)\n",
    "pipeline.train(loaders.loaderTrain, loaders.loaderValid, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c7c95-4726-493d-a887-7e9e86fc8e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm predictions3.json\n",
    "\n",
    "train_preds, train_mAP = pipeline.evaluate(loaders.loaderTrain, 0.5)\n",
    "valid_preds, valid_mAP = pipeline.evaluate(loaders.loaderValid, 0.5)\n",
    "test_preds, _ = pipeline.evaluate(loaders.loaderTest, 0.5, \"predictions3.json\")\n",
    "\n",
    "all_train = eval(open(dataroot3 + \"/train.json\").read())\n",
    "for k in valid_preds:\n",
    "    # We split our training set into train+valid\n",
    "    # so need to remove validation instances from the training set for evaluation\n",
    "    all_train.pop(k)\n",
    "acc3 = accuracy3(all_train, train_preds)\n",
    "print(\"Task 3 training mAP = \" + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f272c0a-cb16-4fea-9fb6-821d546ed7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm predictions3.json\n",
    "# run3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa16bd3-0749-441e-8dc8-5ff46f725112",
   "metadata": {},
   "source": [
    "# Model: Broken AST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871208a-c931-483c-93f2-c82e6849d973",
   "metadata": {},
   "source": [
    "https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8c8589-233b-4bb4-883c-27f2a028e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot3 = \"student_files/task3_audio_classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10501344-33bf-459a-9db9-278a62bde4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_waveform(path):\n",
    "    waveform, sr = librosa.load(dataroot3 + path, sr=SAMPLE_RATE)\n",
    "    waveform = np.array(waveform)\n",
    "    if sr != SAMPLE_RATE:\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "        print(\"resampling\")\n",
    "        waveform = resample(waveform)\n",
    "    # Pad so that everything is the right length\n",
    "    # target_len = SAMPLE_RATE * AUDIO_DURATION\n",
    "    # # waveform = torch.from_numpy(waveform)  # Adding this because I was getting TypeError: pad(): argument 'input' (position 1) must be Tensor, not numpy.ndarray\n",
    "    # if waveform.shape[0] < target_len:\n",
    "    #     print(\"padding\")\n",
    "    #     pad_len = target_len - waveform.shape[0]\n",
    "    #     waveform = F.pad(waveform, (0, pad_len))\n",
    "    # else:\n",
    "    #     waveform = waveform[:target_len]\n",
    "    waveform = torch.FloatTensor(waveform)\n",
    "    \n",
    "    # print(len(waveform))  # should be 160000\n",
    "    if (len(waveform)) != 160000:\n",
    "        print(\"skip this!\")\n",
    "    \n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "592256f4-ae0d-4152-b6d3-79b19f41b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 64\n",
    "N_CLASSES = 10\n",
    "AUDIO_DURATION = 10 # seconds\n",
    "BATCH_SIZE = 4  # From this piazza post: https://piazza.com/class/m8rskujtdvsgy/post/340\n",
    "EPOCHS = 10  # From Discord\n",
    "LEARNING_RATE = 1e-5  # From Discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab786182-2b33-464b-bca2-d01fc4375d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTConfig, AutoFeatureExtractor, ASTForAudioClassification\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "711d07f0-32af-4164-b8b8-43b94ecb7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ASTConfig(num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6770579-f3d6-4683-a65c-dd7e734acd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, meta):\n",
    "        self.meta = meta  # e.g. 'train/0.wav': ['rock']\n",
    "        ks = list(meta.keys())\n",
    "        self.idToPath = dict(zip(range(len(ks)), ks))  # e.g. 0: 'train/0.wav'\n",
    "        self.pathToFeat = {}\n",
    "\n",
    "        for path in ks:\n",
    "            waveform = extract_waveform(path).squeeze(0).numpy()  # Convert to NumPy array\n",
    "            self.pathToFeat[path] = waveform  # e.g. 'train/0.wav': waveform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.idToPath[idx]\n",
    "        tags = self.meta[path]\n",
    "        bin_label = torch.tensor([1 if tag in tags else 0 for tag in TAGS], dtype=torch.float32)\n",
    "        waveform = self.pathToFeat[path]\n",
    "    \n",
    "        return waveform, bin_label, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7312b9d9-480b-4053-a3b2-b7688312c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loaders():\n",
    "    def __init__(self, train_path, test_path, split_ratio=0.9, seed = 0):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        meta_train = eval(open(train_path, 'r').read())\n",
    "        l_test = eval(open(test_path, 'r').read())\n",
    "        meta_test = dict([(x,[]) for x in l_test]) # Need a dictionary for the above class\n",
    "\n",
    "        all_train = AudioDataset(meta_train)\n",
    "        test_set = AudioDataset(meta_test)\n",
    "\n",
    "        # Split all_train into train + valid\n",
    "        total_len = len(all_train)\n",
    "        train_len = int(total_len * split_ratio)\n",
    "        valid_len = total_len - train_len\n",
    "        train_set, valid_set = random_split(all_train, [train_len, valid_len])\n",
    "        \n",
    "        self.loaderTrain = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        self.loaderValid = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        self.loaderTest = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7714f664-dde1-4a70-98d4-7064d16c3202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self, learning_rate, seed = 0):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")  # , config=config\n",
    "        # self.model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\").to(self.device)\n",
    "        self.model = ASTForAudioClassification(config=config).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def evaluate(self, loader, threshold=0.5, outpath=None):\n",
    "        self.model.eval()\n",
    "        preds, targets, paths = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, path in loader:\n",
    "                x = x.to(self.device) #x.cuda()\n",
    "                y = y.to(self.device) #y.cuda()\n",
    "                x = x[0]\n",
    "                y = y[0]\n",
    "                path = path[0]\n",
    "                \n",
    "                # x is a list of raw waveforms\n",
    "                inputs = self.feature_extractor(x, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "                # inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(**inputs).logits\n",
    "                preds.append(torch.sigmoid(logits).cpu())\n",
    "                \n",
    "                y = y.to(self.device)\n",
    "                targets.append(y.cpu())\n",
    "                paths += list(ps)\n",
    "\n",
    "        preds = torch.cat(preds)\n",
    "        targets = torch.cat(targets)\n",
    "        preds_bin = (preds > threshold).float()\n",
    "\n",
    "        predictions = {}\n",
    "        for i in range(preds_bin.shape[0]):\n",
    "            predictions[paths[i]] = [TAGS[j] for j in range(len(preds_bin[i])) if preds_bin[i][j]]\n",
    "\n",
    "        mAP = None\n",
    "        if outpath: # Save predictions\n",
    "            with open(outpath, \"w\") as z:\n",
    "                z.write(str(predictions) + '\\n')\n",
    "        else: # Only compute accuracy if we're *not* saving predictions, since we can't compute test accuracy\n",
    "            mAP = average_precision_score(targets, preds, average='macro')\n",
    "        return predictions, mAP\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for x, y, path in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "                x = x.to(self.device) #x.cuda()\n",
    "                y = y.to(self.device) #y.cuda()\n",
    "                x = x[0]\n",
    "                y = y[0]\n",
    "                path = path[0]\n",
    "                print(x, y, path)\n",
    "                self.optimizer.zero_grad()\n",
    "                inputs = self.feature_extractor(x, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "                outputs = self.model(**inputs).logits\n",
    "                loss = self.criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            val_predictions, mAP = self.evaluate(val_loader)\n",
    "            print(f\"[Epoch {epoch+1}] Loss: {running_loss/len(train_loader):.4f} | Val mAP: {mAP:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c4f5c-d7a5-4330-83ed-06cba2fc3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = Loaders(dataroot3 + \"/train.json\", dataroot3 + \"/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60d1fd-852a-4070-991a-b03fe83bf9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.train(loaders.loaderTrain, loaders.loaderValid, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b450db96-13b2-422a-8663-60efdf35d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_mAP = pipeline.evaluate(loaders.loaderTrain, 0.5)\n",
    "valid_preds, valid_mAP = pipeline.evaluate(loaders.loaderValid, 0.5)\n",
    "test_preds, _ = pipeline.evaluate(loaders.loaderTest, 0.5, \"predictions3.json\")\n",
    "\n",
    "all_train = eval(open(dataroot3 + \"/train.json\").read())\n",
    "for k in valid_preds:\n",
    "    # We split our training set into train+valid\n",
    "    # so need to remove validation instances from the training set for evaluation\n",
    "    all_train.pop(k)\n",
    "acc3 = accuracy3(all_train, train_preds)\n",
    "print(\"Task 3 training mAP = \" + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b29c025-b647-4f79-a4d7-6a5bf051908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'predictions3.json': No such file or directory\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LEARNING_RATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrm predictions3.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m, in \u001b[0;36mrun3\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun3\u001b[39m():\n\u001b[1;32m      2\u001b[0m     loaders \u001b[38;5;241m=\u001b[39m Loaders(dataroot3 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataroot3 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m Pipeline(\u001b[43mLEARNING_RATE\u001b[49m)\n\u001b[1;32m      5\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39mtrain(loaders\u001b[38;5;241m.\u001b[39mloaderTrain, loaders\u001b[38;5;241m.\u001b[39mloaderValid, EPOCHS)\n\u001b[1;32m      6\u001b[0m     train_preds, train_mAP \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mevaluate(loaders\u001b[38;5;241m.\u001b[39mloaderTrain, \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LEARNING_RATE' is not defined"
     ]
    }
   ],
   "source": [
    "!rm predictions3.json\n",
    "run3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e110239-1793-494b-99d7-958da57c6950",
   "metadata": {},
   "source": [
    "# Model: AST Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5274e1b6-852a-4d24-ba82-a34c04bfc9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 20:29:43.902424: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-20 20:29:43.902491: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-20 20:29:43.903774: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-20 20:29:43.910390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import average_precision_score\n",
    "from transformers import ASTConfig, AutoFeatureExtractor, ASTForAudioClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77442c17-89bf-4350-9805-b271a0a78b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a172dd7-1275-4172-ab85-38a8526ba3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants (you can change any of these if useful)\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 64\n",
    "N_CLASSES = 10  # This should be inferred from your TAGS list or dataset\n",
    "AUDIO_DURATION = 10 # seconds\n",
    "BATCH_SIZE = 4  # From this piazza post: https://piazza.com/class/m8rskujtdvsgy/post/340\n",
    "EPOCHS = 8  # 10 From Discord\n",
    "LEARNING_RATE = 1e-5  # From Discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d190350-6c63-4f9d-9dcc-9bc233f06459",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot3 = \"student_files/task3_audio_classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6d8e92-b775-4824-a933-1399b72e1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = ['rock', 'oldies', 'jazz', 'pop', 'dance',  'blues',  'punk', 'chill', 'electronic', 'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4a1815-142e-429b-b15e-0219618fa305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy3(groundtruth, predictions):\n",
    "    preds, targets = [], []\n",
    "    for k in groundtruth:\n",
    "        if not (k in predictions):\n",
    "            print(\"Missing \" + str(k) + \" from predictions\")\n",
    "            return 0\n",
    "        prediction = [1 if tag in predictions[k] else 0 for tag in TAGS]\n",
    "        target = [1 if tag in groundtruth[k] else 0 for tag in TAGS]\n",
    "        preds.append(prediction)\n",
    "        targets.append(target)\n",
    "\n",
    "    mAP = average_precision_score(targets, preds, average='macro')\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109ee7e0-df09-44fd-82d9-ddcaa5157d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_waveform(path, augment=False):\n",
    "    w, sr = librosa.load(dataroot3 + '/' + path, sr=SAMPLE_RATE)\n",
    "\n",
    "    waveform = np.array([w])\n",
    "    \n",
    "    if sr != SAMPLE_RATE:\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "        waveform = resample(waveform)\n",
    "    # Pad so that everything is the right length\n",
    "    target_len = SAMPLE_RATE * AUDIO_DURATION\n",
    "    waveform = torch.from_numpy(waveform)\n",
    "    if waveform.shape[1] < target_len:\n",
    "        pad_len = target_len - waveform.shape[1]\n",
    "        waveform = F.pad(waveform, (0, pad_len))\n",
    "    else:\n",
    "        waveform = waveform[:, :target_len]\n",
    "        \n",
    "    waveform = torch.FloatTensor(waveform)\n",
    "\n",
    "    if not augment:\n",
    "        return waveform\n",
    "\n",
    "    # Data augmentation\n",
    "    rng = random.randint(0, 2)\n",
    "    if rng == 0:\n",
    "        pitch_shift_steps = random.choice([i for i in range(-11, 12) if i != 0])  ## exclude 0 (no pitch change) \n",
    "        augmented_waveform = librosa.effects.pitch_shift(w, sr=sr, n_steps=pitch_shift_steps)\n",
    "    elif rng == 1:\n",
    "        speed_factor = random.uniform(0.5, 1.5)\n",
    "        augmented_waveform = librosa.effects.time_stretch(y=w, rate=speed_factor)\n",
    "    elif rng == 2:\n",
    "        volume_factor = random.uniform(0.5, 2.0)\n",
    "        augmented_waveform = w * volume_factor\n",
    "        # Clip values to prevent distortion/clipping if volume becomes too high\n",
    "        # Audio typically ranges from -1.0 to 1.0 (float32)\n",
    "        augmented_waveform = np.clip(augmented_waveform, -1.0, 1.0)\n",
    "\n",
    "    # Pad so that everything is the right length\n",
    "    augmented_waveform = np.array([w])\n",
    "    augmented_waveform = torch.from_numpy(augmented_waveform)\n",
    "    if augmented_waveform.shape[1] < target_len:\n",
    "        pad_len = target_len - augmented_waveform.shape[1]\n",
    "        augmented_waveform = F.pad(augmented_waveform, (0, pad_len))\n",
    "    else:\n",
    "        augmented_waveform = augmented_waveform[:, :target_len]\n",
    "        \n",
    "    augmented_waveform = torch.FloatTensor(augmented_waveform)\n",
    "\n",
    "    return (waveform, augmented_waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "090dd4e8-d41c-4a09-98c5-41be92f1ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_waveform_pitch_shift(path):\n",
    "#     waveform, sr = librosa.load(dataroot3 + '/' + path, sr=SAMPLE_RATE)\n",
    "\n",
    "#     pitch_shift_steps = random.choice([i for i in range(-11, 12) if i != 0])  ## exclude 0 (no pitch change) \n",
    "#     waveform = librosa.effects.pitch_shift(waveform, sr=sr, n_steps=pitch_shift_steps)\n",
    "    \n",
    "#     waveform = np.array([waveform])\n",
    "    \n",
    "#     waveform = torch.FloatTensor(waveform)\n",
    "    \n",
    "#     return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ca15e0-1858-49cf-96fd-7199250c98e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_waveform_speed_change(path):\n",
    "#     waveform, sr = librosa.load(dataroot3 + '/' + path, sr=SAMPLE_RATE)\n",
    "    \n",
    "#     speed_factor = random.uniform(0.5, 1.5)\n",
    "#     waveform = librosa.effects.time_stretch(y=waveform, rate=speed_factor)\n",
    "\n",
    "#     waveform = np.array([waveform])\n",
    "    \n",
    "#     waveform = torch.FloatTensor(waveform)\n",
    "    \n",
    "#     return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "748064c2-8594-48cf-8296-047a2f172f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_waveform_volume_change(path):\n",
    "#     waveform, sr = librosa.load(dataroot3 + '/' + path, sr=SAMPLE_RATE)\n",
    "\n",
    "#     volume_factor = random.uniform(0.5, 2.0)\n",
    "#     waveform = waveform * volume_factor\n",
    "#     # Clip values to prevent distortion/clipping if volume becomes too high\n",
    "#     # Audio typically ranges from -1.0 to 1.0 (float32)\n",
    "#     augmented_waveform = np.clip(waveform, -1.0, 1.0)\n",
    "\n",
    "#     waveform = np.array([waveform])\n",
    "    \n",
    "#     waveform = torch.FloatTensor(waveform)\n",
    "    \n",
    "#     return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c88b541-bf44-47be-8b86-5312722d6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = ASTConfig(num_labels=N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5653449-b8bb-4806-9d5d-3e76eb070025",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "pretrained_model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20f88ba2-776b-46de-a1d1-b311cffbdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: the code doesn't work if preload is false\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, meta, feature_extractor, augment, preload=True):\n",
    "        self.meta = meta\n",
    "        print(len(meta))\n",
    "        ks = list(meta.keys())\n",
    "        self.idToPath = dict(zip(range(len(ks)), ks))\n",
    "        self.pathToFeat = {}\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.preload = preload\n",
    "\n",
    "        if self.preload:\n",
    "            idx_augment = len(ks)\n",
    "            for path in ks:\n",
    "                # waveform = extract_waveform(path)\n",
    "                # features = self.feature_extractor(waveform.squeeze().numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "                # self.pathToFeat[path] = features['input_values'].squeeze(0)\n",
    "\n",
    "                rng = random.randint(0, 3)\n",
    "                if augment and rng == 0:\n",
    "                    print(idx_augment)\n",
    "                    # A quarter of a chance of data being augmented\n",
    "                    waveforms = extract_waveform(path, augment=True)\n",
    "                    \n",
    "                    waveform = waveforms[0]\n",
    "                    features = self.feature_extractor(waveform.squeeze().numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "                    self.pathToFeat[path] = features['input_values'].squeeze(0)\n",
    "                    \n",
    "                    augmented_waveform = waveforms[1]\n",
    "                    features = self.feature_extractor(augmented_waveform.squeeze().numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "                    self.pathToFeat[path + \"_augment\"] = features['input_values'].squeeze(0)\n",
    "                    self.idToPath[idx_augment] = path + \"_augment\"\n",
    "                    self.meta[path + \"_augment\"] = self.meta[path]\n",
    "                    idx_augment += 1\n",
    "                else:\n",
    "                    waveform = extract_waveform(path)\n",
    "                    features = self.feature_extractor(waveform.squeeze().numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "                    self.pathToFeat[path] = features['input_values'].squeeze(0)\n",
    "\n",
    "            if augment:\n",
    "                idx_augment = len(ks)\n",
    "                for path in ks:\n",
    "                    \n",
    "                    idx_augment += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.idToPath[idx]\n",
    "        tags = self.meta[path]\n",
    "        bin_label = torch.tensor([1 if tag in tags else 0 for tag in TAGS], dtype=torch.float32)\n",
    "\n",
    "        if self.preload:\n",
    "            features = self.pathToFeat[path]\n",
    "        else:\n",
    "            waveform = extract_waveform(path)\n",
    "            features = self.feature_extractor(waveform.squeeze().numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "            features = features['input_values'].squeeze(0)\n",
    "\n",
    "        return features, bin_label, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c354bbe7-7ee8-4baf-8358-7184115a849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loaders():\n",
    "    def __init__(self, train_path, test_path, feature_extractor, split_ratio=0.9, seed=0):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        meta_train = eval(open(train_path, 'r').read())\n",
    "        l_test = eval(open(test_path, 'r').read())\n",
    "        meta_test = dict([(x, []) for x in l_test])\n",
    "\n",
    "        all_train = AudioDataset(meta_train, feature_extractor, augment=True)\n",
    "        test_set = AudioDataset(meta_test, feature_extractor, augment=False)\n",
    "\n",
    "        total_len = len(all_train)\n",
    "        train_len = int(total_len * split_ratio)\n",
    "        valid_len = total_len - train_len\n",
    "        train_set, valid_set = random_split(all_train, [train_len, valid_len])\n",
    "\n",
    "        self.loaderTrain = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        self.loaderValid = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        self.loaderTest = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30dc1c00-f297-4cf9-9a89-31fc9ee198aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTForMultiLabel(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model  # e.g. ASTForAudioClassification.from_pretrained(...)\n",
    "        # self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(527, num_labels)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.base_model(input_values).logits  # shape: [B, 527]\n",
    "        # outputs = self.dropout(outputs)\n",
    "        outputs = self.classifier(outputs)              # shape: [B, num_labels]\n",
    "        return torch.sigmoid(outputs)                   # For multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b44ce89-f4e9-4bec-bac4-49b38ddadeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self, model, learning_rate, seed=0):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def evaluate(self, loader, threshold=0.5, outpath=None):\n",
    "        print(\"evaluating\")\n",
    "        self.model.eval()\n",
    "        preds, targets, paths = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, ps in loader:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                outputs = self.model(x)\n",
    "                preds.append(outputs.cpu())\n",
    "                targets.append(y.cpu())\n",
    "                paths += list(ps)\n",
    "\n",
    "        preds = torch.cat(preds)\n",
    "        targets = torch.cat(targets)\n",
    "        preds_bin = (preds > threshold).float()\n",
    "\n",
    "        predictions = {}\n",
    "        for i in range(preds_bin.shape[0]):\n",
    "            predictions[paths[i]] = [TAGS[j] for j in range(len(preds_bin[i])) if preds_bin[i][j]]\n",
    "\n",
    "        mAP = None\n",
    "        if outpath:\n",
    "            with open(outpath, \"w\") as z:\n",
    "                z.write(str(predictions) + '\\n')\n",
    "        else:\n",
    "            mAP = average_precision_score(targets, preds, average='macro')\n",
    "        return predictions, mAP\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs):\n",
    "        print(\"training\")\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for x, y, path in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(x)\n",
    "                loss = self.criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            val_predictions, mAP = self.evaluate(val_loader)\n",
    "            print(f\"[Epoch {epoch+1}] Loss: {running_loss/len(train_loader):.4f} | Val mAP: {mAP:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc55f168-bbcc-44c0-9ac1-d0fbed3b02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy3(all_train_meta, train_preds):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for audio_file, true_tags in all_train_meta.items():\n",
    "        if audio_file in train_preds:\n",
    "            predicted_tags = set(train_preds[audio_file])\n",
    "            true_tags_set = set(true_tags)\n",
    "            if predicted_tags == true_tags_set:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "    if total_predictions > 0:\n",
    "        return correct_predictions / total_predictions\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c469373d-3505-4155-97b7-b952ecdf419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run3():\n",
    "#     print(\"here 1\")\n",
    "#     loaders = Loaders(dataroot3 + \"/train.json\", dataroot3 + \"/test.json\", feature_extractor)\n",
    "#     print(\"here 2\")\n",
    "#     model = ASTForMultiLabel(pretrained_model, N_CLASSES)\n",
    "#     print(\"here 3\")\n",
    "#     pipeline = Pipeline(model, LEARNING_RATE)\n",
    "#     print(\"here 4\")\n",
    "\n",
    "#     pipeline.train(loaders.loaderTrain, loaders.loaderValid, EPOCHS)\n",
    "#     train_preds, train_mAP = pipeline.evaluate(loaders.loaderTrain, 0.5)\n",
    "#     valid_preds, valid_mAP = pipeline.evaluate(loaders.loaderValid, 0.5)\n",
    "#     test_preds, _ = pipeline.evaluate(loaders.loaderTest, 0.5, \"predictions3.json\")\n",
    "\n",
    "#     all_train = eval(open(dataroot3 + \"/train.json\").read())\n",
    "#     for k in valid_preds:\n",
    "#         if k in all_train: # Ensure the key exists before trying to pop\n",
    "#             all_train.pop(k)\n",
    "#     acc3 = accuracy3(all_train, train_preds)\n",
    "#     print(\"Task 3 training accuracy (exact match) = \" + str(acc3))\n",
    "#     print(\"Task 3 training mAP = \" + str(train_mAP))\n",
    "#     print(\"Task 3 validation mAP = \" + str(valid_mAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbb23d94-6f9f-4e55-8c3d-c232a3cc3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm predictions3.json\n",
    "# run3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f49e1084-ec37-40a1-ba7f-1e272a7785e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here 1\n",
      "4000\n",
      "4000\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n",
      "4013\n",
      "4014\n",
      "4015\n",
      "4016\n",
      "4017\n",
      "4018\n",
      "4019\n",
      "4020\n",
      "4021\n",
      "4022\n",
      "4023\n",
      "4024\n",
      "4025\n",
      "4026\n",
      "4027\n",
      "4028\n",
      "4029\n",
      "4030\n",
      "4031\n",
      "4032\n",
      "4033\n",
      "4034\n",
      "4035\n",
      "4036\n",
      "4037\n",
      "4038\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4042\n",
      "4043\n",
      "4044\n",
      "4045\n",
      "4046\n",
      "4047\n",
      "4048\n",
      "4049\n",
      "4050\n",
      "4051\n",
      "4052\n",
      "4053\n",
      "4054\n",
      "4055\n",
      "4056\n",
      "4057\n",
      "4058\n",
      "4059\n",
      "4060\n",
      "4061\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4067\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n",
      "4507\n",
      "4508\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4512\n",
      "4513\n",
      "4514\n",
      "4515\n",
      "4516\n",
      "4517\n",
      "4518\n",
      "4519\n",
      "4520\n",
      "4521\n",
      "4522\n",
      "4523\n",
      "4524\n",
      "4525\n",
      "4526\n",
      "4527\n",
      "4528\n",
      "4529\n",
      "4530\n",
      "4531\n",
      "4532\n",
      "4533\n",
      "4534\n",
      "4535\n",
      "4536\n",
      "4537\n",
      "4538\n",
      "4539\n",
      "4540\n",
      "4541\n",
      "4542\n",
      "4543\n",
      "4544\n",
      "4545\n",
      "4546\n",
      "4547\n",
      "4548\n",
      "4549\n",
      "4550\n",
      "4551\n",
      "4552\n",
      "4553\n",
      "4554\n",
      "4555\n",
      "4556\n",
      "4557\n",
      "4558\n",
      "4559\n",
      "4560\n",
      "4561\n",
      "4562\n",
      "4563\n",
      "4564\n",
      "4565\n",
      "4566\n",
      "4567\n",
      "4568\n",
      "4569\n",
      "4570\n",
      "4571\n",
      "4572\n",
      "4573\n",
      "4574\n",
      "4575\n",
      "4576\n",
      "4577\n",
      "4578\n",
      "4579\n",
      "4580\n",
      "4581\n",
      "4582\n",
      "4583\n",
      "4584\n",
      "4585\n",
      "4586\n",
      "4587\n",
      "4588\n",
      "4589\n",
      "4590\n",
      "4591\n",
      "4592\n",
      "4593\n",
      "4594\n",
      "4595\n",
      "4596\n",
      "4597\n",
      "4598\n",
      "4599\n",
      "4600\n",
      "4601\n",
      "4602\n",
      "4603\n",
      "4604\n",
      "4605\n",
      "4606\n",
      "4607\n",
      "4608\n",
      "4609\n",
      "4610\n",
      "4611\n",
      "4612\n",
      "4613\n",
      "4614\n",
      "4615\n",
      "4616\n",
      "4617\n",
      "4618\n",
      "4619\n",
      "4620\n",
      "4621\n",
      "4622\n",
      "4623\n",
      "4624\n",
      "4625\n",
      "4626\n",
      "4627\n",
      "4628\n",
      "4629\n",
      "4630\n",
      "4631\n",
      "4632\n",
      "4633\n",
      "4634\n",
      "4635\n",
      "4636\n",
      "4637\n",
      "4638\n",
      "4639\n",
      "4640\n",
      "4641\n",
      "4642\n",
      "4643\n",
      "4644\n",
      "4645\n",
      "4646\n",
      "4647\n",
      "4648\n",
      "4649\n",
      "4650\n",
      "4651\n",
      "4652\n",
      "4653\n",
      "4654\n",
      "4655\n",
      "4656\n",
      "4657\n",
      "4658\n",
      "4659\n",
      "4660\n",
      "4661\n",
      "4662\n",
      "4663\n",
      "4664\n",
      "4665\n",
      "4666\n",
      "4667\n",
      "4668\n",
      "4669\n",
      "4670\n",
      "4671\n",
      "4672\n",
      "4673\n",
      "4674\n",
      "4675\n",
      "4676\n",
      "4677\n",
      "4678\n",
      "4679\n",
      "4680\n",
      "4681\n",
      "4682\n",
      "4683\n",
      "4684\n",
      "4685\n",
      "4686\n",
      "4687\n",
      "4688\n",
      "4689\n",
      "4690\n",
      "4691\n",
      "4692\n",
      "4693\n",
      "4694\n",
      "4695\n",
      "4696\n",
      "4697\n",
      "4698\n",
      "4699\n",
      "4700\n",
      "4701\n",
      "4702\n",
      "4703\n",
      "4704\n",
      "4705\n",
      "4706\n",
      "4707\n",
      "4708\n",
      "4709\n",
      "4710\n",
      "4711\n",
      "4712\n",
      "4713\n",
      "4714\n",
      "4715\n",
      "4716\n",
      "4717\n",
      "4718\n",
      "4719\n",
      "4720\n",
      "4721\n",
      "4722\n",
      "4723\n",
      "4724\n",
      "4725\n",
      "4726\n",
      "4727\n",
      "4728\n",
      "4729\n",
      "4730\n",
      "4731\n",
      "4732\n",
      "4733\n",
      "4734\n",
      "4735\n",
      "4736\n",
      "4737\n",
      "4738\n",
      "4739\n",
      "4740\n",
      "4741\n",
      "4742\n",
      "4743\n",
      "4744\n",
      "4745\n",
      "4746\n",
      "4747\n",
      "4748\n",
      "4749\n",
      "4750\n",
      "4751\n",
      "4752\n",
      "4753\n",
      "4754\n",
      "4755\n",
      "4756\n",
      "4757\n",
      "4758\n",
      "4759\n",
      "4760\n",
      "4761\n",
      "4762\n",
      "4763\n",
      "4764\n",
      "4765\n",
      "4766\n",
      "4767\n",
      "4768\n",
      "4769\n",
      "4770\n",
      "4771\n",
      "4772\n",
      "4773\n",
      "4774\n",
      "4775\n",
      "4776\n",
      "4777\n",
      "4778\n",
      "4779\n",
      "4780\n",
      "4781\n",
      "4782\n",
      "4783\n",
      "4784\n",
      "4785\n",
      "4786\n",
      "4787\n",
      "4788\n",
      "4789\n",
      "4790\n",
      "4791\n",
      "4792\n",
      "4793\n",
      "4794\n",
      "4795\n",
      "4796\n",
      "4797\n",
      "4798\n",
      "4799\n",
      "4800\n",
      "4801\n",
      "4802\n",
      "4803\n",
      "4804\n",
      "4805\n",
      "4806\n",
      "4807\n",
      "4808\n",
      "4809\n",
      "4810\n",
      "4811\n",
      "4812\n",
      "4813\n",
      "4814\n",
      "4815\n",
      "4816\n",
      "4817\n",
      "4818\n",
      "4819\n",
      "4820\n",
      "4821\n",
      "4822\n",
      "4823\n",
      "4824\n",
      "4825\n",
      "4826\n",
      "4827\n",
      "4828\n",
      "4829\n",
      "4830\n",
      "4831\n",
      "4832\n",
      "4833\n",
      "4834\n",
      "4835\n",
      "4836\n",
      "4837\n",
      "4838\n",
      "4839\n",
      "4840\n",
      "4841\n",
      "4842\n",
      "4843\n",
      "4844\n",
      "4845\n",
      "4846\n",
      "4847\n",
      "4848\n",
      "4849\n",
      "4850\n",
      "4851\n",
      "4852\n",
      "4853\n",
      "4854\n",
      "4855\n",
      "4856\n",
      "4857\n",
      "4858\n",
      "4859\n",
      "4860\n",
      "4861\n",
      "4862\n",
      "4863\n",
      "4864\n",
      "4865\n",
      "4866\n",
      "4867\n",
      "4868\n",
      "4869\n",
      "4870\n",
      "4871\n",
      "4872\n",
      "4873\n",
      "4874\n",
      "4875\n",
      "4876\n",
      "4877\n",
      "4878\n",
      "4879\n",
      "4880\n",
      "4881\n",
      "4882\n",
      "4883\n",
      "4884\n",
      "4885\n",
      "4886\n",
      "4887\n",
      "4888\n",
      "4889\n",
      "4890\n",
      "4891\n",
      "4892\n",
      "4893\n",
      "4894\n",
      "4895\n",
      "4896\n",
      "4897\n",
      "4898\n",
      "4899\n",
      "4900\n",
      "4901\n",
      "4902\n",
      "4903\n",
      "4904\n",
      "4905\n",
      "4906\n",
      "4907\n",
      "4908\n",
      "4909\n",
      "4910\n",
      "4911\n",
      "4912\n",
      "4913\n",
      "4914\n",
      "4915\n",
      "4916\n",
      "4917\n",
      "4918\n",
      "4919\n",
      "4920\n",
      "4921\n",
      "4922\n",
      "4923\n",
      "4924\n",
      "4925\n",
      "4926\n",
      "4927\n",
      "4928\n",
      "4929\n",
      "4930\n",
      "4931\n",
      "4932\n",
      "4933\n",
      "4934\n",
      "4935\n",
      "4936\n",
      "4937\n",
      "4938\n",
      "4939\n",
      "4940\n",
      "4941\n",
      "4942\n",
      "4943\n",
      "4944\n",
      "4945\n",
      "4946\n",
      "4947\n",
      "4948\n",
      "4949\n",
      "4950\n",
      "4951\n",
      "4952\n",
      "4953\n",
      "4954\n",
      "4955\n",
      "4956\n",
      "4957\n",
      "4958\n",
      "4959\n",
      "4960\n",
      "4961\n",
      "4962\n",
      "4963\n",
      "4964\n",
      "4965\n",
      "4966\n",
      "4967\n",
      "4968\n",
      "4969\n",
      "4970\n",
      "4971\n",
      "4972\n",
      "4973\n",
      "4974\n",
      "4975\n",
      "4976\n",
      "4977\n",
      "4978\n",
      "4979\n",
      "4980\n",
      "4981\n",
      "4982\n",
      "4983\n",
      "4984\n",
      "4985\n",
      "4986\n",
      "4987\n",
      "4988\n",
      "4989\n",
      "4990\n",
      "4991\n",
      "4992\n",
      "4993\n",
      "4994\n",
      "4995\n",
      "4996\n",
      "4997\n",
      "4998\n",
      "4999\n",
      "5000\n",
      "5001\n",
      "5002\n",
      "5003\n",
      "5004\n",
      "1000\n",
      "here 2\n",
      "here 3\n",
      "here 4\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1126/1126 [07:28<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "[Epoch 1] Loss: 0.1933 | Val mAP: 0.6880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1126/1126 [07:30<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "[Epoch 2] Loss: 0.0938 | Val mAP: 0.7755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1126/1126 [07:30<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "[Epoch 3] Loss: 0.0384 | Val mAP: 0.8222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1126/1126 [07:30<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "[Epoch 4] Loss: 0.0198 | Val mAP: 0.8423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1126/1126 [07:30<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "[Epoch 5] Loss: 0.0112 | Val mAP: 0.8441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1126/1126 [07:30<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "[Epoch 6] Loss: 0.0121 | Val mAP: 0.8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1126/1126 [07:28<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "[Epoch 7] Loss: 0.0095 | Val mAP: 0.8506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1126/1126 [07:30<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "[Epoch 8] Loss: 0.0052 | Val mAP: 0.8539\n"
     ]
    }
   ],
   "source": [
    "print(\"here 1\")\n",
    "loaders = Loaders(dataroot3 + \"/train.json\", dataroot3 + \"/test.json\", feature_extractor)\n",
    "# print(\"here 2\")\n",
    "# model = ASTForMultiLabel(pretrained_model, N_CLASSES)\n",
    "# print(\"here 3\")\n",
    "# pipeline = Pipeline(model, LEARNING_RATE)\n",
    "# print(\"here 4\")\n",
    "# pipeline.train(loaders.loaderTrain, loaders.loaderValid, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95a01886-adc5-4910-a357-3bce548e3dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved to: trained_ast_model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"trained_ast_model.pth\")\n",
    "print(f\"Trained model saved to: trained_ast_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3e3de-050f-4c57-8b0c-da5eb0e7c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"ast_model_full.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de2f0cdf-371c-445c-9e9c-4ec1897d0b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    }
   ],
   "source": [
    "test_preds, _ = pipeline.evaluate(loaders.loaderTest, 0.5, \"predictions3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a69ac81a-5684-4ad4-8f60-f5aa58284349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "evaluating\n",
      "Task 3 training accuracy (exact match) = 0.9744869661674986\n",
      "Task 3 training mAP = 0.9996161692415464\n",
      "Task 3 validation mAP = 0.8538579134996767\n"
     ]
    }
   ],
   "source": [
    "train_preds, train_mAP = pipeline.evaluate(loaders.loaderTrain, 0.5)\n",
    "valid_preds, valid_mAP = pipeline.evaluate(loaders.loaderValid, 0.5)\n",
    "\n",
    "all_train = eval(open(dataroot3 + \"/train.json\").read())\n",
    "for k in valid_preds:\n",
    "    if k in all_train: # Ensure the key exists before trying to pop\n",
    "        all_train.pop(k)\n",
    "acc3 = accuracy3(all_train, train_preds)\n",
    "print(\"Task 3 training accuracy (exact match) = \" + str(acc3))\n",
    "print(\"Task 3 training mAP = \" + str(train_mAP))\n",
    "print(\"Task 3 validation mAP = \" + str(valid_mAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf7fc9-05d6-4022-8815-c52347d9b80c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
